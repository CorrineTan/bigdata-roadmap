22/12/12 01:05:14 INFO SparkContext: Running Spark version 3.3.1
22/12/12 01:05:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/12/12 01:05:15 INFO ResourceUtils: ==============================================================
22/12/12 01:05:15 INFO ResourceUtils: No custom resources configured for spark.driver.
22/12/12 01:05:15 INFO ResourceUtils: ==============================================================
22/12/12 01:05:15 INFO SparkContext: Submitted application: RatingsHistogram
22/12/12 01:05:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
22/12/12 01:05:15 INFO ResourceProfile: Limiting resource is cpu
22/12/12 01:05:15 INFO ResourceProfileManager: Added ResourceProfile id: 0
22/12/12 01:05:15 INFO SecurityManager: Changing view acls to: corrtan
22/12/12 01:05:15 INFO SecurityManager: Changing modify acls to: corrtan
22/12/12 01:05:15 INFO SecurityManager: Changing view acls groups to:
22/12/12 01:05:15 INFO SecurityManager: Changing modify acls groups to:
22/12/12 01:05:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(corrtan); groups with view permissions: Set(); users  with modify permissions: Set(corrtan); groups with modify permissions: Set()
22/12/12 01:05:15 INFO Utils: Successfully started service 'sparkDriver' on port 56923.
22/12/12 01:05:15 INFO SparkEnv: Registering MapOutputTracker
22/12/12 01:05:15 INFO SparkEnv: Registering BlockManagerMaster
22/12/12 01:05:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/12/12 01:05:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/12/12 01:05:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/12/12 01:05:15 INFO DiskBlockManager: Created local directory at /private/var/folders/7t/bjqk97n134b27z0qyr68_wq80000gs/T/blockmgr-7651f697-b89a-42e9-8d10-424d6bf41453
22/12/12 01:05:15 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
22/12/12 01:05:15 INFO SparkEnv: Registering OutputCommitCoordinator
22/12/12 01:05:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/12/12 01:05:15 INFO Executor: Starting executor ID driver on host 192.168.0.230
22/12/12 01:05:15 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
22/12/12 01:05:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56924.
22/12/12 01:05:15 INFO NettyBlockTransferService: Server created on 192.168.0.230:56924
22/12/12 01:05:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/12/12 01:05:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.230, 56924, None)
22/12/12 01:05:15 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.230:56924 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.230, 56924, None)
22/12/12 01:05:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.230, 56924, None)
22/12/12 01:05:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.230, 56924, None)
22/12/12 01:05:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.4 KiB, free 434.2 MiB)
22/12/12 01:05:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 434.2 MiB)
22/12/12 01:05:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.230:56924 (size: 32.5 KiB, free: 434.4 MiB)
22/12/12 01:05:16 INFO SparkContext: Created broadcast 0 from textFile at DirectMethodHandleAccessor.java:104
Traceback (most recent call last):
  File "/Users/corrtan/Desktop/SparkCourse/ratings-counter.py", line 10, in <module>
    result = ratings.countByValue()
  File "/usr/local/Cellar/apache-spark/3.3.1/libexec/python/lib/pyspark.zip/pyspark/rdd.py", line 1755, in countByValue
  File "/usr/local/Cellar/apache-spark/3.3.1/libexec/python/lib/pyspark.zip/pyspark/rdd.py", line 1250, in reduce
  File "/usr/local/Cellar/apache-spark/3.3.1/libexec/python/lib/pyspark.zip/pyspark/rdd.py", line 1197, in collect
  File "/usr/local/Cellar/apache-spark/3.3.1/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/usr/local/Cellar/apache-spark/3.3.1/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py", line 326, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/ml-100k/u.data
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
	at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1020)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
	at java.base/java.lang.reflect.Method.invoke(Method.java:578)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1589)
Caused by: java.io.IOException: Input path does not exist: file:/ml-100k/u.data
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)
	... 32 more

22/12/12 01:05:16 INFO SparkContext: Invoking stop() from shutdown hook
22/12/12 01:05:16 INFO SparkUI: Stopped Spark web UI at http://192.168.0.230:4040
22/12/12 01:05:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/12/12 01:05:16 INFO MemoryStore: MemoryStore cleared
22/12/12 01:05:16 INFO BlockManager: BlockManager stopped
22/12/12 01:05:16 INFO BlockManagerMaster: BlockManagerMaster stopped
22/12/12 01:05:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/12/12 01:05:16 INFO SparkContext: Successfully stopped SparkContext
22/12/12 01:05:16 INFO ShutdownHookManager: Shutdown hook called
22/12/12 01:05:16 INFO ShutdownHookManager: Deleting directory /private/var/folders/7t/bjqk97n134b27z0qyr68_wq80000gs/T/spark-50a8c662-efa7-4051-a80f-8c777955d77a/pyspark-b970f688-8d8f-4d75-904f-79d5e3a11260
22/12/12 01:05:16 INFO ShutdownHookManager: Deleting directory /private/var/folders/7t/bjqk97n134b27z0qyr68_wq80000gs/T/spark-5c6953b7-0844-4024-9942-2d02d250720f
22/12/12 01:05:16 INFO ShutdownHookManager: Deleting directory /private/var/folders/7t/bjqk97n134b27z0qyr68_wq80000gs/T/spark-50a8c662-efa7-4051-a80f-8c777955d77a
    ~/Desktop/SparkCourse ▓▒░ spark-submit ratings-counter.py                                                     ░▒▓ 1 ✘  5s   base  
22/12/12 01:08:58 INFO SparkContext: Running Spark version 3.3.1
22/12/12 01:08:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/12/12 01:08:58 INFO ResourceUtils: ==============================================================
22/12/12 01:08:58 INFO ResourceUtils: No custom resources configured for spark.driver.
22/12/12 01:08:58 INFO ResourceUtils: ==============================================================
22/12/12 01:08:58 INFO SparkContext: Submitted application: RatingsHistogram
22/12/12 01:08:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
22/12/12 01:08:58 INFO ResourceProfile: Limiting resource is cpu
22/12/12 01:08:58 INFO ResourceProfileManager: Added ResourceProfile id: 0
22/12/12 01:08:58 INFO SecurityManager: Changing view acls to: corrtan
22/12/12 01:08:58 INFO SecurityManager: Changing modify acls to: corrtan
22/12/12 01:08:58 INFO SecurityManager: Changing view acls groups to:
22/12/12 01:08:58 INFO SecurityManager: Changing modify acls groups to:
22/12/12 01:08:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(corrtan); groups with view permissions: Set(); users  with modify permissions: Set(corrtan); groups with modify permissions: Set()
22/12/12 01:08:59 INFO Utils: Successfully started service 'sparkDriver' on port 57095.
22/12/12 01:08:59 INFO SparkEnv: Registering MapOutputTracker
22/12/12 01:08:59 INFO SparkEnv: Registering BlockManagerMaster
22/12/12 01:08:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/12/12 01:08:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/12/12 01:08:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/12/12 01:08:59 INFO DiskBlockManager: Created local directory at /private/var/folders/7t/bjqk97n134b27z0qyr68_wq80000gs/T/blockmgr-7f14fbe3-9e2a-42a7-944f-fcf650f74252
22/12/12 01:08:59 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
22/12/12 01:08:59 INFO SparkEnv: Registering OutputCommitCoordinator
22/12/12 01:08:59 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/12/12 01:08:59 INFO Executor: Starting executor ID driver on host 192.168.0.230
22/12/12 01:08:59 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
22/12/12 01:08:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57097.
22/12/12 01:08:59 INFO NettyBlockTransferService: Server created on 192.168.0.230:57097
22/12/12 01:08:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/12/12 01:08:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.230, 57097, None)
22/12/12 01:08:59 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.230:57097 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.230, 57097, None)
22/12/12 01:08:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.230, 57097, None)
22/12/12 01:08:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.230, 57097, None)
22/12/12 01:09:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.4 KiB, free 434.2 MiB)
22/12/12 01:09:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 434.2 MiB)
22/12/12 01:09:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.230:57097 (size: 32.5 KiB, free: 434.4 MiB)
22/12/12 01:09:00 INFO SparkContext: Created broadcast 0 from textFile at DirectMethodHandleAccessor.java:104
22/12/12 01:09:00 INFO FileInputFormat: Total input files to process : 1
22/12/12 01:09:00 INFO SparkContext: Starting job: countByValue at /Users/corrtan/Desktop/SparkCourse/ratings-counter.py:10
22/12/12 01:09:00 INFO DAGScheduler: Got job 0 (countByValue at /Users/corrtan/Desktop/SparkCourse/ratings-counter.py:10) with 1 output partitions
22/12/12 01:09:00 INFO DAGScheduler: Final stage: ResultStage 0 (countByValue at /Users/corrtan/Desktop/SparkCourse/ratings-counter.py:10)
22/12/12 01:09:00 INFO DAGScheduler: Parents of final stage: List()
22/12/12 01:09:00 INFO DAGScheduler: Missing parents: List()
22/12/12 01:09:00 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at countByValue at /Users/corrtan/Desktop/SparkCourse/ratings-counter.py:10), which has no missing parents
22/12/12 01:09:00 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.5 KiB, free 434.1 MiB)
22/12/12 01:09:00 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 434.1 MiB)
22/12/12 01:09:00 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.230:57097 (size: 5.7 KiB, free: 434.4 MiB)
22/12/12 01:09:00 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
22/12/12 01:09:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at countByValue at /Users/corrtan/Desktop/SparkCourse/ratings-counter.py:10) (first 15 tasks are for partitions Vector(0))
22/12/12 01:09:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
22/12/12 01:09:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.230, executor driver, partition 0, PROCESS_LOCAL, 4523 bytes) taskResourceAssignments Map()
22/12/12 01:09:01 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/12/12 01:09:01 INFO HadoopRDD: Input split: file:/Users/corrtan/Desktop/SparkCourse/ml-100k/u.data:0+1979173
22/12/12 01:09:02 INFO PythonRunner: Times: total = 1177, boot = 1028, init = 21, finish = 128
22/12/12 01:09:02 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1536 bytes result sent to driver
22/12/12 01:09:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1393 ms on 192.168.0.230 (executor driver) (1/1)
22/12/12 01:09:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/12/12 01:09:02 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 57098
22/12/12 01:09:02 INFO DAGScheduler: ResultStage 0 (countByValue at /Users/corrtan/Desktop/SparkCourse/ratings-counter.py:10) finished in 1.493 s
22/12/12 01:09:02 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/12/12 01:09:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/12/12 01:09:02 INFO DAGScheduler: Job 0 finished: countByValue at /Users/corrtan/Desktop/SparkCourse/ratings-counter.py:10, took 1.531887 s
1 6110
2 11370
3 27145
4 34174
5 21201
22/12/12 01:09:02 INFO SparkContext: Invoking stop() from shutdown hook
22/12/12 01:09:02 INFO SparkUI: Stopped Spark web UI at http://192.168.0.230:4040
22/12/12 01:09:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/12/12 01:09:02 INFO MemoryStore: MemoryStore cleared
22/12/12 01:09:02 INFO BlockManager: BlockManager stopped
22/12/12 01:09:02 INFO BlockManagerMaster: BlockManagerMaster stopped
22/12/12 01:09:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/12/12 01:09:02 INFO SparkContext: Successfully stopped SparkContext
22/12/12 01:09:02 INFO ShutdownHookManager: Shutdown hook called
22/12/12 01:09:02 INFO ShutdownHookManager: Deleting directory /private/var/folders/7t/bjqk97n134b27z0qyr68_wq80000gs/T/spark-74636810-ddf5-4e06-b5ae-4a9a15fdef09
22/12/12 01:09:02 INFO ShutdownHookManager: Deleting directory /private/var/folders/7t/bjqk97n134b27z0qyr68_wq80000gs/T/spark-3cd0226e-d6fe-4f8f-92ef-ab38b4a3c558/pyspark-dc5b6e66-f32a-414f-abeb-5eef902ada30
22/12/12 01:09:02 INFO ShutdownHookManager: Deleting directory /private/var/folders/7t/bjqk97n134b27z0qyr68_wq80000gs/T/spark-3cd0226e-d6fe-4f8f-92ef-ab38b4a3c558